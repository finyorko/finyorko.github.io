<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[微信防撤回脚本]]></title>
    <url>%2F3.html</url>
    <content type="text"><![CDATA[Windows微信防撤回脚本 测试版本：2.6.7.57 ~ 2.6.8.51 下载地址：https://pan.baiduwp.com/s/1UBpRXRyd9uXBeqDbc0DKWg 提取码: 6666 使用方法：将下载的dll文件，放置在微信的安装目录（WeChat.exe所在目录）下，默认安装目录（C:\Program Files(x86)\Tencent\WeChat），如果在安装微信时，自定义了微信安装位置，请自行查找。（如果有问题，右下角蓝色聊天框戳我） 测试结果： 组件制作过程 使用工具：x32dbg调试器 打开微信，并登录； 打开x32dbg，文件-附加-选择“微信”，如下图； 符号-搜索关键词：“win”，选择wechatwin.dll模块，然后双击 跳转至“引用”区域 ； 右键搜索-当前模块-字符串，输入搜索关键次“revokemsg”，选择 mov.ecx.wechatwin.xxxxxx 字段结果，双击定位至详情 从定位直至上拉，第一一个je开头行，修改成jmp开头行，点确定]]></content>
      <categories>
        <category>防撤回</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[网页下载百度云文件]]></title>
    <url>%2F2.html</url>
    <content type="text"><![CDATA[pandownload网页版正式发布，一款完全免费且支持全平台免登录、免安装的下载工具。 解析地址1 解析地址2 在分享链接的baidu后面加上wp可以快速跳转到网页版，例如： 分享链接： https://pan.baidu.com/s/1f78tODfx-1g_qTrg4fhVoA 提取码: 6666 网页版链接： https://pan.baiduwp.com/s/1f78tODfx-1g_qTrg4fhVoA]]></content>
  </entry>
  <entry>
    <title><![CDATA[秘迹搜索]]></title>
    <url>%2F1.html</url>
    <content type="text"><![CDATA[一个不受追踪的搜索引擎（堪比谷歌） 秘迹]]></content>
  </entry>
  <entry>
    <title><![CDATA[基于tensorflow的人脸识别系统]]></title>
    <url>%2F4.html</url>
    <content type="text"><![CDATA[环境搭建 ubuntu16.04+Anaconda4.2.0+python3.5+opencv2的环境搭建 代码分析 检验导包问题的代码 12345678910111213141516import tensorflow as tf# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点# 加到默认图中.# 构造器的返回值代表该常量 op 的返回值.##matrix1 = tf.constant([[3., 3.]])# 创建另外一个常量 op, 产生一个 2x1 矩阵.##matrix2 = tf.constant([[2.],[2.]])# 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.# 返回值 'product' 代表矩阵乘法的结果.##product = tf.matmul(matrix1,matrix2)from skimage import io, transformimport globimport osimport time import tensorflow as tfimport numpy as np 将训练集下的图片resize一下，伴随着打开每一个子文件夹，我们要为其设置一个labels，一个文件夹的1000应一个label，共68个abel，这是后面检验acc的唯一指标，即是否能把测试集的照片通过我们的网络输出到指定出口得到正确的label。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189path=''#将所有的图片resize成100*100w=128h=128c=3#读取图片def read_img(path): cate=[path+'/'+x for x in os.listdir(path) if os.path.isdir(path+'/'+x)] imgs=[] labels=[] for idx,folder in enumerate(cate): for im in glob.glob(folder+'/*.png'): print('reading the images:%s'%(im)) img=io.imread(im) img=transform.resize(img,(w,h,c)) imgs.append(img) labels.append(idx) return np.asarray(imgs,np.float32),np.asarray(labels,np.int32) data,label=read_img(path)# 将所有数据分为训练集和验证集ratio = 0.95#训练集占比s = np.int ( num_example * ratio )x_train = data[:s]y_train = label[:s]x_val = data[s:]y_val = label[s:]# -----------------构建网络----------------------# 占位符x = tf.placeholder ( tf.float32, shape=[None, w, h, c], name='x' )y_ = tf.placeholder ( tf.int32, shape=[None, ], name='y_' )def CNNlayer(): # 第一个卷积层（128——&gt;64) conv1 = tf.layers.conv2d ( inputs=x, filters=32, kernel_size=[5, 5], padding="same", activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer ( stddev=0.01 ) ) pool1 = tf.layers.max_pooling2d ( inputs=conv1, pool_size=[2, 2], strides=2 ) # 第二个卷积层(64-&gt;32) conv2 = tf.layers.conv2d ( inputs=pool1, filters=64, kernel_size=[5, 5], padding="same", activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer ( stddev=0.01 ) ) pool2 = tf.layers.max_pooling2d ( inputs=conv2, pool_size=[2, 2], strides=2 ) # 第三个卷积层(32-&gt;16) conv3 = tf.layers.conv2d ( inputs=pool2, filters=128, kernel_size=[3, 3], padding="same", activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer ( stddev=0.01 ) ) pool3 = tf.layers.max_pooling2d ( inputs=conv3, pool_size=[2, 2], strides=2 ) # 第四个卷积层(16-&gt;8) conv4 = tf.layers.conv2d ( inputs=pool3, filters=128, kernel_size=[3, 3], padding="same", activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer ( stddev=0.01 ) ) pool4 = tf.layers.max_pooling2d ( inputs=conv4, pool_size=[2, 2], strides=2 ) re1 = tf.reshape ( pool4, [-1, 8 * 8 * 128] ) # 全连接层 dense1 = tf.layers.dense ( inputs=re1, units=1024, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer ( stddev=0.01 ), kernel_regularizer=tf.contrib.layers.l2_regularizer ( 0.003 ) ) dense2 = tf.layers.dense ( inputs=dense1, units=512, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer ( stddev=0.01 ), kernel_regularizer=tf.contrib.layers.l2_regularizer ( 0.003 ) ) logits = tf.layers.dense ( inputs=dense2, units=68, activation=None, kernel_initializer=tf.truncated_normal_initializer ( stddev=0.01 ), kernel_regularizer=tf.contrib.layers.l2_regularizer ( 0.003 ) ) return logits# ----------网络结束---------------------------# 定义一个函数，按批次取数据def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False): assert len ( inputs ) == len ( targets ) if shuffle: indices = np.arange ( len ( inputs ) ) np.random.shuffle ( indices ) for start_idx in range ( 0, len ( inputs ) - batch_size + 1, batch_size ): if shuffle: excerpt = indices[start_idx:start_idx + batch_size] else: excerpt = slice ( start_idx, start_idx + batch_size ) yield inputs[excerpt], targets[excerpt]# 训练和测试数据，可将n_epoch设置更大一些saver = tf.train.Saver ( max_to_keep=3 )max_acc = 0f = open ( 'ckpt1/acc.txt', 'w' )n_epoch = 10batch_size = 64sess = tf.InteractiveSession ( )sess.run ( tf.global_variables_initializer ( ) )for epoch in range ( n_epoch ): start_time = time.time ( ) # training train_loss, train_acc, n_batch = 0, 0, 0 for x_train_a, y_train_a in minibatches ( x_train, y_train, batch_size, shuffle=True ): _, err, ac = sess.run ( [train_op, loss, acc], feed_dict=&#123;x: x_train_a, y_: y_train_a&#125; ) train_loss += err; train_acc += ac; n_batch += 1 print ( " train loss: %f" % (train_loss / n_batch) ) print ( " train acc: %f" % (train_acc / n_batch) ) # validation val_loss, val_acc, n_batch = 0, 0, 0 for x_val_a, y_val_a in minibatches ( x_val, y_val, batch_size, shuffle=False ): err, ac = sess.run ( [loss, acc], feed_dict=&#123;x: x_val_a, y_: y_val_a&#125; ) val_loss += err; val_acc += ac; n_batch += 1 print ( " validation loss: %f" % (val_loss / n_batch) ) print ( " validation acc: %f" % (val_acc / n_batch) ) f.write ( str ( epoch + 1 ) + ', val_acc: ' + str ( val_acc ) + '\n' ) if val_acc &gt; max_acc: max_acc = val_acc saver.save ( sess, 'ckpt1/faces.ckpt', global_step=epoch + 1 )f.close ( )detector = dlib.get_frontal_face_detector ( ) # 获取人脸分类器ID = (1511346,1610731,1610763,1610260,1611407,1611408, 1611409,1611412,1611413,1611415,1611417,1611418, 1611419,1611420,1611421,1611424,1611425,1611426, 1611427,1611430,1611431,1611433,1611434,1611436, 1611437,1611438,1611440,1611444,1611446,1611447, 1611449,1611450,1611451,1511453,1611455,1611458, 1611459,1611460,1611461,1611462,1611470,1611471, 1611472,1611472,1611476,1611478,1611480,1611482, 1611483,1611486,1611487,1611488,1611490,1611491, 1611492,1611493,1611494,1613371,1613376,1613378, 1613550,1711459 )#两个操作是拿到dlib的人脸分类器（相当于dlib的训练代码跑完的结果存下的参数变量结构等东西），然后建个数组当输出和ID的映射#最终交互检验：user = input ( "图片（G）还是摄像头（V）:" )if user == "G": path = input ( "图片路径名是：" ) img = cv2.imread ( path ) dets = detector ( img, 1 ) print ( "Number of faces detected: &#123;&#125;".format ( len ( dets ) ) ) for index, face in enumerate ( dets ): print ( 'face &#123;&#125;; left &#123;&#125;; top &#123;&#125;; right &#123;&#125;; bottom &#123;&#125;'.format ( index, face.left ( ), face.top ( ), face.right ( ), face.bottom ( ) ) ) left = face.left ( ) top = face.top ( ) right = face.right ( ) bottom = face.bottom ( ) cv2.rectangle ( img, (left, top), (right, bottom), (0, 255, 0), 3 ) io.imsave ( 'temp.png', img ) img1 = io.imread ( 'temp.png' ) img1 = transform.resize ( img1, (w, h, c) ) cv2.imshow ( 'image', img1 ) img1 = img[top:bottom, left:right] img1 = transform.resize ( img1, (w, h, c) ) # cv2.imshow('image1',img) res = sess.run ( predict, feed_dict=&#123;x: [img1]&#125; ) print ( ID[res[0]] ) if len ( dets ) == 0: img = transform.resize ( img, (w, h, c) ) res = sess.run ( predict, feed_dict=&#123;x: [img]&#125; ) print ( ID[res[0]] ) cv2.waitKey ( 0 ) cv2.destroyAllWindows ( ) cv2.waitKey ( 0 ) cv2.destroyAllWindows ( )else: # 打开摄像头 cap = cv2.VideoCapture ( 0 ) # 视屏封装格式 while True: ret, frame = cap.read ( ) gray = cv2.cvtColor ( frame, cv2.COLOR_BGR2GRAY ) cv2.imshow ( 'frame', frame ) # 抓取图像，s画人脸框，q结束识别 if cv2.waitKey ( 1 ) &amp; 0xFF == ord ( 's' ): cv2.imwrite ( 'now.png', frame ) img = cv2.imread ( "now.png" ) dets = detector ( img, 1 ) print ( "Number of faces detected: &#123;&#125;".format ( len ( dets ) ) ) for index, face in enumerate ( dets ): print ( 'face &#123;&#125;; left &#123;&#125;; top &#123;&#125;; right &#123;&#125;; bottom &#123;&#125;'.format ( index, face.left ( ), face.top ( ), face.right ( ), face.bottom ( ) ) ) left = face.left ( ) top = face.top ( ) right = face.right ( ) bottom = face.bottom ( ) img = img[top:bottom, left:right] # img=io.imread('image/now.png') img = transform.resize ( img, (w, h, c) ) res = sess.run ( predict, feed_dict=&#123;x: [img]&#125; ) print ( ID[res[0]] ) # 退出 实验结果分析 最终的训练的结果： 根据我自己的数据化的数据图： 识别过程：（拿我自己的照片做的测试）：]]></content>
      <categories>
        <category>Python</category>
        <category>人脸识别</category>
      </categories>
      <tags>
        <tag>人脸识别</tag>
      </tags>
  </entry>
</search>
